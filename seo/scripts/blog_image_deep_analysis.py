#!/usr/bin/env python3
"""
Derinlemesine Blog GÃ¶rsel Optimizasyonu
Tematik Ã§eÅŸitlilik ve anlam bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ saÄŸlama
"""

import os
import json
import re
from datetime import datetime
from bs4 import BeautifulSoup
from collections import Counter

class BlogImageAnalyzer:
    def __init__(self, project_root="/Users/macos/Documents/Projeler/DryAlle"):
        self.project_root = project_root
        self.blog_root = os.path.join(project_root, 'blog')
        
        # Tematik kategoriler ve anahtar kelimeler
        self.themes = {
            "perde": ["perde", "tÃ¼l", "stor", "pencere", "cam", "kapÄ±", "ev tekstili", "dekorasyon"],
            "mobilya": ["koltuk", "kanepe", "kumaÅŸ", "dÃ¶ÅŸeme", "sandalye", "berjer", "minder", "yastÄ±k"],
            "halÄ±": ["halÄ±", "kilim", "yÃ¼n", "el dokuma", "kaymaz", "antika", "vintage", "dokuma"],
            "leke": ["leke Ã§Ä±karma", "yaÄŸ lekesi", "kahve lekesi", "kan lekesi", "mÃ¼rekkep", "pas", "temizlik"],
            "gelinlik": ["gelinlik", "dÃ¼ÄŸÃ¼n", "Ã¶zel gÃ¼n", "abiye", "tÃ¼vit", "gece elbisesi", "kostÃ¼m"],
            "deri": ["deri", "sÃ¼et", "nubuk", "vintage deri", "deri ceket", "ayakkabÄ±", "Ã§anta"],
            "ipek": ["ipek", "saten", "ÅŸifon", "organze", "brokat", "jakarlÄ±", "kadife"],
            "yÃ¼n": ["yÃ¼n", "kaÅŸmir", "angora", "yÃ¼nlÃ¼", "triko", "kazak", "mont", "palto"],
            "profesyonel": ["kuru temizleme", "profesyonel", "makine", "kimyasal", "uzman", "teknisyen"]
        }
        
        # Kurumsal renkler ve stil gereksinimleri
        self.brand_requirements = {
            "primary_colors": ["#006a44", "#f6ec3d", "#e1e1e1", "#f0f0f0"],
            "style": "temiz, profesyonel, gÃ¼venilir",
            "mood": "temizlik, kalite, gÃ¼ven"
        }

    def analyze_blog_content(self, blog_dir, html_path):
        """Blog iÃ§eriÄŸini derinlemesine analiz et"""
        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # BaÅŸlÄ±k ve meta bilgileri
            title = soup.find('title')
            title_text = title.get_text() if title else ""
            
            meta_desc = soup.find('meta', {'name': 'description'})
            description = meta_desc.get('content', '') if meta_desc else ""
            
            # Ana iÃ§erik
            main_content = ""
            for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'li']):
                main_content += tag.get_text() + " "
            
            # TÃ¼m metni temizle ve analiz et
            full_text = (title_text + " " + description + " " + main_content).lower()
            
            # Tema analizi
            theme_scores = {}
            for theme, keywords in self.themes.items():
                score = sum(full_text.count(keyword) for keyword in keywords)
                if score > 0:
                    theme_scores[theme] = score
            
            # Birincil tema
            primary_theme = max(theme_scores, key=theme_scores.get) if theme_scores else "genel"
            
            # Anahtar kelime Ã§Ä±karÄ±mÄ±
            words = re.findall(r'\b\w{4,}\b', full_text)
            stop_words = {'iÃ§in', 'olan', 'ile', 'bir', 'bu', 'ÅŸu', 'olarak', 'kadar', 'sonra', 'Ã¶nce', 'sÄ±rasÄ±nda', 'esnasÄ±nda', 'sebebiyle', 'nedeniyle'}
            filtered_words = [w for w in words if w not in stop_words and not w.isdigit()]
            
            word_counts = Counter(filtered_words)
            top_keywords = [word for word, count in word_counts.most_common(8) if count > 1]
            
            # Ã–zel gereksinimler
            special_requirements = []
            
            # Lokasyon bazlÄ±
            if any(loc in full_text for loc in ['istanbul', 'ankara', 'izmir']):
                special_requirements.append("TÃ¼rkiye lokasyonu")
            
            # Sezon bazlÄ±
            if any(season in full_text for season in ['kÄ±ÅŸ', 'yaz', 'sonbahar', 'ilkbahar']):
                special_requirements.append("Mevsimsel")
            
            # Ã–zel durum
            if any(special in full_text for special in ['acil', 'hÄ±zlÄ±', '24 saat', 'express']):
                special_requirements.append("HÄ±zlÄ± servis")
            
            # Mevcut gÃ¶rsel analizi
            current_image = soup.find('img', {'class': 'featured-image'})
            current_image_info = {
                'exists': current_image is not None,
                'src': current_image.get('src', '') if current_image else '',
                'alt': current_image.get('alt', '') if current_image else '',
                'size_info': self.get_image_size_info(blog_dir, current_image.get('src', '')) if current_image else None
            }
            
            return {
                'slug': blog_dir,
                'title': title_text.strip(),
                'description': description.strip(),
                'primary_theme': primary_theme,
                'theme_scores': theme_scores,
                'keywords': top_keywords[:5],
                'special_requirements': special_requirements,
                'content_length': len(main_content.split()),
                'current_image': current_image_info,
                'seo_analysis': {
                    'has_title': bool(title_text),
                    'has_description': bool(description),
                    'title_length': len(title_text),
                    'description_length': len(description)
                }
            }
            
        except Exception as e:
            print(f"âŒ Error analyzing {blog_dir}: {str(e)}")
            return None

    def get_image_size_info(self, blog_dir, img_src):
        """Mevcut gÃ¶rsel boyut bilgisini al"""
        if not img_src:
            return None
            
        img_path = os.path.join(self.blog_root, blog_dir, img_src)
        if os.path.exists(img_path):
            try:
                from PIL import Image
                with Image.open(img_path) as img:
                    return {
                        'width': img.width,
                        'height': img.height,
                        'format': img.format,
                        'size_kb': os.path.getsize(img_path) // 1024
                    }
            except Exception:
                return {'error': 'Cannot read image'}
        return None

    def generate_image_requirements(self, analysis):
        """Her blog iÃ§in gÃ¶rsel gereksinimleri oluÅŸtur"""
        requirements = {
            'primary_subject': analysis['primary_theme'],
            'context': 'kuru temizleme, profesyonel temizlik',
            'style': 'temiz, modern, profesyonel',
            'color_harmony': self.brand_requirements['primary_colors'],
            'mood': self.brand_requirements['mood']
        }
        
        # Tema bazlÄ± Ã¶zel gereksinimler
        theme_specific = {
            'perde': {
                'subjects': ['perde temizliÄŸi', 'ev tekstili', 'pencere dekorasyonu'],
                'settings': ['ev ortamÄ±', 'modern salon', 'temiz pencere'],
                'props': ['temiz perde', 'Ã¼tÃ¼', 'temizlik malzemeleri']
            },
            'mobilya': {
                'subjects': ['koltuk temizliÄŸi', 'kumaÅŸ bakÄ±mÄ±', 'dÃ¶ÅŸeme temizliÄŸi'],
                'settings': ['modern salon', 'oturma odasÄ±', 'temiz ev'],
                'props': ['temiz koltuk', 'temizlik ekipmanÄ±', 'kumaÅŸ bakÄ±m Ã¼rÃ¼nleri']
            },
            'halÄ±': {
                'subjects': ['halÄ± temizliÄŸi', 'kilim bakÄ±mÄ±', 'zemin temizliÄŸi'],
                'settings': ['ev iÃ§i', 'salon', 'temiz zemin'],
                'props': ['temiz halÄ±', 'temizlik makinesi', 'bakÄ±m Ã¼rÃ¼nleri']
            },
            'leke': {
                'subjects': ['leke Ã§Ä±karma', 'temizlik iÅŸlemi', 'Ã¶nce-sonra'],
                'settings': ['temizlik atÃ¶lyesi', 'profesyonel ortam'],
                'props': ['temizlik Ã¼rÃ¼nleri', 'leke Ã§Ä±karma malzemeleri']
            },
            'gelinlik': {
                'subjects': ['gelinlik bakÄ±mÄ±', 'Ã¶zel gÃ¼n hazÄ±rlÄ±ÄŸÄ±', 'nazik temizlik'],
                'settings': ['butik ortamÄ±', 'temiz, ÅŸÄ±k mekan'],
                'props': ['beyaz gelinlik', 'Ã¶zenli ambalaj', 'premium hizmet']
            }
        }
        
        if analysis['primary_theme'] in theme_specific:
            requirements.update(theme_specific[analysis['primary_theme']])
        
        # Anahtar kelime bazlÄ± ek gereksinimler
        for keyword in analysis['keywords']:
            if keyword in ['istanbul', 'ankara', 'izmir']:
                requirements['location'] = f'{keyword} ÅŸehir manzarasÄ±'
            elif keyword in ['acil', 'hÄ±zlÄ±', 'express']:
                requirements['urgency'] = 'hÄ±zlÄ± servis, acil durum'
            elif keyword in ['premium', 'lÃ¼ks', 'Ã¶zel']:
                requirements['quality_level'] = 'premium, lÃ¼ks hizmet'
        
        return requirements

    def create_manual_selection_data(self):
        """Manuel seÃ§im iÃ§in veri hazÄ±rla"""
        print("ğŸ“Š Blog iÃ§eriklerini analiz ediliyor...")
        
        all_analyses = []
        
        for blog_dir in os.listdir(self.blog_root):
            if blog_dir.startswith('.') or blog_dir == 'index.html':
                continue
            
            blog_path = os.path.join(self.blog_root, blog_dir)
            index_path = os.path.join(blog_path, 'index.html')
            
            if os.path.isdir(blog_path) and os.path.exists(index_path):
                analysis = self.analyze_blog_content(blog_dir, index_path)
                if analysis:
                    # GÃ¶rsel gereksinimleri ekle
                    analysis['image_requirements'] = self.generate_image_requirements(analysis)
                    all_analyses.append(analysis)
        
        return all_analyses

    def generate_image_search_queries(self, analysis):
        """Her blog iÃ§in Ã¶zel arama sorgularÄ± oluÅŸtur"""
        base_queries = []
        
        # Temel tema sorgularÄ±
        theme = analysis['primary_theme']
        base_queries.extend([
            f"{theme} cleaning professional",
            f"{theme} care service",
            f"dry cleaning {theme}",
            f"professional {theme} maintenance"
        ])
        
        # Anahtar kelime kombinasyonlarÄ±
        for keyword in analysis['keywords'][:3]:
            if len(keyword) > 3:
                base_queries.append(f"{keyword} professional cleaning")
        
        # Ã–zel durum sorgularÄ±
        if 'TÃ¼rkiye lokasyonu' in analysis['special_requirements']:
            base_queries.append(f"{theme} cleaning Turkey Istanbul")
        
        if 'HÄ±zlÄ± servis' in analysis['special_requirements']:
            base_queries.append(f"fast {theme} cleaning service")
        
        # GÃ¶rsel tarzÄ± sorgularÄ±
        base_queries.extend([
            f"clean modern {theme}",
            f"professional laundry {theme}",
            f"before after {theme} cleaning"
        ])
        
        return base_queries[:8]  # En fazla 8 sorgu

    def create_selection_csv(self, analyses):
        """Manuel seÃ§im iÃ§in CSV dosyasÄ± oluÅŸtur"""
        csv_content = "Slug,Tema,BaÅŸlÄ±k,Anahtar Kelimeler,Ã–zel Gereksinimler,Mevcut GÃ¶rsel,GÃ¶rsel Gereksinimleri,Arama SorgularÄ±,SeÃ§ilen GÃ¶rsel URL,Notlar\n"
        
        for analysis in analyses:
            slug = analysis['slug']
            theme = analysis['primary_theme']
            title = analysis['title'].replace(',', ';')
            keywords = '; '.join(analysis['keywords'])
            requirements = '; '.join(analysis['special_requirements'])
            current_img = analysis['current_image']['src'] if analysis['current_image']['exists'] else 'Yok'
            
            # GÃ¶rsel gereksinimleri Ã¶zet
            img_req = analysis['image_requirements']
            req_summary = f"Konu: {img_req['primary_subject']}, Stil: {img_req['style']}"
            
            # Arama sorgularÄ±
            search_queries = self.generate_image_search_queries(analysis)
            queries_text = '; '.join(search_queries)
            
            csv_content += f'"{slug}","{theme}","{title}","{keywords}","{requirements}","{current_img}","{req_summary}","{queries_text}","",""\n'
        
        return csv_content

    def generate_theme_distribution_report(self, analyses):
        """Tema daÄŸÄ±lÄ±m raporu oluÅŸtur"""
        theme_distribution = {}
        for analysis in analyses:
            theme = analysis['primary_theme']
            theme_distribution[theme] = theme_distribution.get(theme, 0) + 1
        
        return {
            'total_blogs': len(analyses),
            'theme_distribution': theme_distribution,
            'diversity_score': len(theme_distribution) / len(analyses) * 100,
            'recommendations': self.get_diversity_recommendations(theme_distribution)
        }

    def get_diversity_recommendations(self, theme_distribution):
        """Ã‡eÅŸitlilik Ã¶nerileri oluÅŸtur"""
        recommendations = []
        
        # En Ã§ok kullanÄ±lan tema
        most_common = max(theme_distribution, key=theme_distribution.get)
        most_count = theme_distribution[most_common]
        
        if most_count > len(theme_distribution) * 0.4:  # %40'tan fazla
            recommendations.append(f"'{most_common}' temasÄ± Ã§ok baskÄ±n ({most_count} blog). Ã‡eÅŸitlilik iÃ§in diÄŸer temalar gÃ¼Ã§lendirilmeli.")
        
        # Az kullanÄ±lan temalar
        rare_themes = [theme for theme, count in theme_distribution.items() if count == 1]
        if rare_themes:
            recommendations.append(f"Tek blog ile temsil edilen temalar: {', '.join(rare_themes)}")
        
        # Eksik temalar
        all_possible_themes = set(self.themes.keys())
        used_themes = set(theme_distribution.keys())
        missing_themes = all_possible_themes - used_themes
        
        if missing_themes:
            recommendations.append(f"HiÃ§ kullanÄ±lmayan temalar: {', '.join(missing_themes)}")
        
        return recommendations

    def generate_comprehensive_report(self):
        """KapsamlÄ± analiz raporu oluÅŸtur"""
        print("ğŸ“Š KapsamlÄ± blog gÃ¶rsel analizi baÅŸlÄ±yor...")
        
        # TÃ¼m analizleri yap
        analyses = self.create_manual_selection_data()
        
        # Tema daÄŸÄ±lÄ±m analizi
        theme_report = self.generate_theme_distribution_report(analyses)
        
        # Manuel seÃ§im CSV'si oluÅŸtur
        csv_content = self.create_selection_csv(analyses)
        
        # Ana rapor
        report = {
            'analysis_date': datetime.now().isoformat(),
            'project': 'DryAlle Blog GÃ¶rsel Optimizasyonu',
            'total_blogs_analyzed': len(analyses),
            'theme_distribution': theme_report,
            'detailed_analyses': analyses,
            'optimization_priorities': self.get_optimization_priorities(analyses),
            'next_steps': [
                'Manuel gÃ¶rsel seÃ§imi iÃ§in CSV dosyasÄ±nÄ± incele',
                'Her blog iÃ§in en uygun 3 gÃ¶rsel adayÄ±nÄ± belirle', 
                'SeÃ§ilen gÃ¶rselleri WebP formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r',
                'HTML dosyalarÄ±nÄ± gÃ¼ncelle',
                'Lighthouse performans testi yap'
            ]
        }
        
        # DosyalarÄ± kaydet
        analysis_path = os.path.join(self.project_root, 'seo/reports/blog_image_analysis.json')
        csv_path = os.path.join(self.project_root, 'seo/reports/image_selection.csv')
        
        os.makedirs(os.path.dirname(analysis_path), exist_ok=True)
        
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write(csv_content)
        
        return analysis_path, csv_path, report

    def get_optimization_priorities(self, analyses):
        """Optimizasyon Ã¶nceliklerini belirle"""
        priorities = []
        
        # GÃ¶rseli olmayan bloglar
        no_image_blogs = [a for a in analyses if not a['current_image']['exists']]
        if no_image_blogs:
            priorities.append({
                'level': 'CRITICAL',
                'issue': f'{len(no_image_blogs)} blog hiÃ§ gÃ¶rseli yok',
                'blogs': [b['slug'] for b in no_image_blogs]
            })
        
        # DÃ¼ÅŸÃ¼k kaliteli gÃ¶rseller
        low_quality_blogs = []
        for a in analyses:
            if a['current_image']['exists']:
                size_info = a['current_image'].get('size_info')
                if size_info and size_info.get('size_kb', 0) > 200:
                    low_quality_blogs.append(a['slug'])
        
        if low_quality_blogs:
            priorities.append({
                'level': 'HIGH',
                'issue': f'{len(low_quality_blogs)} blog bÃ¼yÃ¼k boyutlu gÃ¶rsel kullanÄ±yor (>200KB)',
                'blogs': low_quality_blogs
            })
        
        # Tema uyumsuzluÄŸu (bu aÅŸamada analiz edemiyoruz, placeholder)
        priorities.append({
            'level': 'MEDIUM',
            'issue': 'TÃ¼m bloglar iÃ§in tema-gÃ¶rsel uyumluluÄŸu kontrol edilmeli',
            'blogs': [a['slug'] for a in analyses]
        })
        
        return priorities

def main():
    """Blog GÃ¶rsel Analizi ve Optimizasyon PlanÄ±"""
    print("ğŸ–¼ï¸ BLOG GÃ–RSEL OPTÄ°MÄ°ZASYONU - DERÄ°NLEMESÄ°NE ANALÄ°Z")
    print("=" * 70)
    print("ğŸ¯ Tematik Ã‡eÅŸitlilik | Anlam BÃ¼tÃ¼nlÃ¼ÄŸÃ¼ | GÃ¶rsel Kalite")
    print("=" * 70)
    
    analyzer = BlogImageAnalyzer()
    
    try:
        # KapsamlÄ± analiz
        analysis_path, csv_path, report = analyzer.generate_comprehensive_report()
        
        # Ã–zet
        print("\n" + "=" * 70)
        print("ğŸ–¼ï¸ BLOG GÃ–RSEL ANALÄ°ZÄ° TAMAMLANDI")
        print("=" * 70)
        
        print(f"âœ… Analiz edilen blog sayÄ±sÄ±: {report['total_blogs_analyzed']}")
        
        # Tema daÄŸÄ±lÄ±mÄ±
        theme_dist = report['theme_distribution']['theme_distribution']
        print(f"âœ… Tema daÄŸÄ±lÄ±mÄ±:")
        for theme, count in sorted(theme_dist.items(), key=lambda x: x[1], reverse=True):
            print(f"   {theme}: {count} blog")
        
        # Ã‡eÅŸitlilik skoru
        diversity = report['theme_distribution']['diversity_score']
        print(f"âœ… Ã‡eÅŸitlilik skoru: {diversity:.1f}%")
        
        # Ã–ncelikli sorunlar
        priorities = report['optimization_priorities']
        critical_issues = [p for p in priorities if p['level'] == 'CRITICAL']
        if critical_issues:
            print(f"\nğŸš¨ KRÄ°TÄ°K SORUNLAR:")
            for issue in critical_issues:
                print(f"   â€¢ {issue['issue']}")
        
        # Ã–neriler
        recommendations = report['theme_distribution']['recommendations']
        if recommendations:
            print(f"\nğŸ’¡ Ã–NERÄ°LER:")
            for i, rec in enumerate(recommendations[:3], 1):
                print(f"   {i}. {rec}")
        
        print(f"\nğŸ“‹ MANUEL SEÃ‡Ä°M Ä°Ã‡Ä°N:")
        print(f"   ğŸ“Š DetaylÄ± Analiz: {analysis_path}")
        print(f"   ğŸ“ SeÃ§im Tablosu: {csv_path}")
        
        print(f"\nğŸ”„ SONRAKI ADIMLAR:")
        for i, step in enumerate(report['next_steps'][:3], 1):
            print(f"   {i}. {step}")
        
        print(f"\nğŸ’» MANUEL KONTROL GEREKSÄ°NÄ°MÄ°:")
        print(f"   â€¢ CSV dosyasÄ±nÄ± aÃ§Ä±n: {csv_path}")
        print(f"   â€¢ Her blog iÃ§in 'SeÃ§ilen GÃ¶rsel URL' sÃ¼tununu doldurun")
        print(f"   â€¢ ArdÄ±ndan gÃ¶rsel indirme ve optimizasyon scriptini Ã§alÄ±ÅŸtÄ±rÄ±n")
        
        return True
        
    except Exception as e:
        print(f"âŒ Analiz hatasÄ±: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)