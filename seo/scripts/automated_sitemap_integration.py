#!/usr/bin/env python3
"""
Otomatik Sitemap Y√∂netimi Sistemi (D2)
GitHub Actions entegrasyonu ile dinamik sitemap olu≈üturma

√ñzellikler:
- Otomatik sitemap g√ºncellemesi (yeni blog yayƒ±nlandƒ±ƒüƒ±nda)
- Dinamik √∂ncelik belirleme
- W3C sitemap standartlarƒ± uyumluluƒüu
- Google Search Console entegrasyonu hazƒ±rlƒ±ƒüƒ±
"""

import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import json
import re
from urllib.parse import quote

class AutomatedSitemapManager:
    def __init__(self, project_root="/Users/macos/Documents/Projeler/DryAlle"):
        self.project_root = project_root
        self.blog_root = os.path.join(project_root, 'blog')
        self.base_url = "https://dryallekurutemizleme.com"
        
        # Sitemap √∂ncelik sistemi
        self.priority_rules = {
            'homepage': {'priority': '1.0', 'changefreq': 'daily'},
            'main_pages': {'priority': '0.9', 'changefreq': 'weekly'},
            'service_pages': {'priority': '0.8', 'changefreq': 'monthly'},
            'blog_index': {'priority': '0.8', 'changefreq': 'weekly'},
            'blog_posts': {'priority': '0.6', 'changefreq': 'monthly'},
            'location_pages': {'priority': '0.7', 'changefreq': 'monthly'},
            'other_pages': {'priority': '0.5', 'changefreq': 'yearly'}
        }
        
        # XML namespace'ler
        self.namespaces = {
            'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9',
            'mobile': 'http://www.google.com/schemas/sitemap-mobile/1.0',
            'image': 'http://www.google.com/schemas/sitemap-image/1.1'
        }

    def detect_published_blogs(self):
        """Yayƒ±nlanmƒ±≈ü blog postlarƒ±nƒ± tespit et"""
        published_blogs = []
        
        for item in os.listdir(self.blog_root):
            item_path = os.path.join(self.blog_root, item)
            if os.path.isdir(item_path) and not item.startswith('.'):
                index_path = os.path.join(item_path, 'index.html')
                
                if os.path.exists(index_path):
                    # Blog status kontrol√º
                    blog_status = self.check_blog_status(index_path)
                    
                    if blog_status['published']:
                        published_blogs.append({
                            'slug': item,
                            'path': index_path,
                            'url': f"{self.base_url}/blog/{item}/",
                            'lastmod': blog_status['lastmod'],
                            'images': blog_status['images']
                        })
        
        return published_blogs

    def check_blog_status(self, html_path):
        """Blog yayƒ±n durumunu kontrol et"""
        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Draft kontrol√º
            is_draft = any([
                'draft: true' in content,
                'status: draft' in content,
                'published: false' in content,
                '_draft' in html_path
            ])
            
            # Son deƒüi≈üiklik tarihi
            lastmod = datetime.fromtimestamp(os.path.getmtime(html_path)).strftime('%Y-%m-%d')
            
            # G√∂rselleri tespit et
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(content, 'html.parser')
            images = []
            
            # Featured image
            featured_img = soup.find('img', {'class': 'featured-image'})
            if featured_img and featured_img.get('src'):
                slug = os.path.basename(os.path.dirname(html_path))
                if featured_img['src'].startswith('http'):
                    images.append(featured_img['src'])
                else:
                    images.append(f"{self.base_url}/blog/{slug}/{featured_img['src']}")
            
            return {
                'published': not is_draft,
                'lastmod': lastmod,
                'images': images
            }
            
        except Exception as e:
            print(f"‚ùå Blog status kontrol hatasƒ± {html_path}: {str(e)}")
            return {
                'published': True,  # Hata durumunda yayƒ±nlanmƒ±≈ü kabul et
                'lastmod': datetime.now().strftime('%Y-%m-%d'),
                'images': []
            }

    def detect_all_site_pages(self):
        """Sitedeki t√ºm sayfalarƒ± tespit et"""
        all_pages = []
        
        # Ana sayfa
        all_pages.append({
            'url': f"{self.base_url}/",
            'priority': self.priority_rules['homepage']['priority'],
            'changefreq': self.priority_rules['homepage']['changefreq'],
            'lastmod': datetime.now().strftime('%Y-%m-%d'),
            'type': 'homepage'
        })
        
        # Ana sayfalar
        main_pages = ['sss.html', 'iletisim.html', 'hizmetler.html']
        for page in main_pages:
            page_path = os.path.join(self.project_root, page)
            if os.path.exists(page_path):
                all_pages.append({
                    'url': f"{self.base_url}/{page}",
                    'priority': self.priority_rules['main_pages']['priority'],
                    'changefreq': self.priority_rules['main_pages']['changefreq'],
                    'lastmod': datetime.fromtimestamp(os.path.getmtime(page_path)).strftime('%Y-%m-%d'),
                    'type': 'main_page'
                })
        
        # Blog index
        blog_index_path = os.path.join(self.blog_root, 'index.html')
        if os.path.exists(blog_index_path):
            all_pages.append({
                'url': f"{self.base_url}/blog/",
                'priority': self.priority_rules['blog_index']['priority'],
                'changefreq': self.priority_rules['blog_index']['changefreq'],
                'lastmod': datetime.fromtimestamp(os.path.getmtime(blog_index_path)).strftime('%Y-%m-%d'),
                'type': 'blog_index'
            })
        
        # Diƒüer HTML sayfalarƒ±nƒ± tara
        self.scan_additional_pages(all_pages)
        
        return all_pages

    def scan_additional_pages(self, all_pages):
        """Ek sayfalarƒ± tara (hizmetler, b√∂lgeler vb.)"""
        excluded_dirs = ['.git', 'node_modules', 'seo', 'blog', '.github']
        
        for root, dirs, files in os.walk(self.project_root):
            # Excluded klas√∂rleri atla
            dirs[:] = [d for d in dirs if d not in excluded_dirs]
            
            for file in files:
                if file.endswith('.html') and file != 'index.html':
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, self.project_root)
                    
                    # URL olu≈ütur
                    url = f"{self.base_url}/{relative_path}"
                    
                    # Sayfa tipini belirle
                    page_type = self.determine_page_type(relative_path)
                    
                    all_pages.append({
                        'url': url,
                        'priority': self.priority_rules[page_type]['priority'],
                        'changefreq': self.priority_rules[page_type]['changefreq'],
                        'lastmod': datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d'),
                        'type': page_type
                    })

    def determine_page_type(self, relative_path):
        """Sayfa tipini belirle"""
        if 'hizmet' in relative_path.lower():
            return 'service_pages'
        elif 'bolge' in relative_path.lower() or 'semt' in relative_path.lower():
            return 'location_pages'
        else:
            return 'other_pages'

    def create_comprehensive_sitemap(self, site_pages, blog_pages):
        """Kapsamlƒ± sitemap olu≈ütur"""
        print("üó∫Ô∏è Kapsamlƒ± sitemap olu≈üturuluyor...")
        
        # XML root element
        urlset = ET.Element('urlset')
        for prefix, uri in self.namespaces.items():
            urlset.set(f'xmlns:{prefix}' if prefix != 'sitemap' else 'xmlns', uri)
        
        total_urls = 0
        
        # Site sayfalarƒ±nƒ± ekle
        for page in site_pages:
            url_elem = self.create_url_element(page)
            urlset.append(url_elem)
            total_urls += 1
        
        # Blog sayfalarƒ±nƒ± ekle
        for blog in blog_pages:
            blog_page = {
                'url': blog['url'],
                'priority': self.priority_rules['blog_posts']['priority'],
                'changefreq': self.priority_rules['blog_posts']['changefreq'],
                'lastmod': blog['lastmod'],
                'images': blog['images'],
                'type': 'blog_post'
            }
            
            url_elem = self.create_url_element(blog_page, include_images=True)
            urlset.append(url_elem)
            total_urls += 1
        
        print(f"‚úÖ {total_urls} URL sitemap'e eklendi")
        
        return ET.ElementTree(urlset)

    def create_url_element(self, page_data, include_images=False):
        """URL elementi olu≈ütur"""
        url_elem = ET.Element('url')
        
        # Location
        loc = ET.SubElement(url_elem, 'loc')
        loc.text = page_data['url']
        
        # Last modification
        lastmod = ET.SubElement(url_elem, 'lastmod')
        lastmod.text = page_data['lastmod']
        
        # Change frequency
        changefreq = ET.SubElement(url_elem, 'changefreq')
        changefreq.text = page_data['changefreq']
        
        # Priority
        priority = ET.SubElement(url_elem, 'priority')
        priority.text = page_data['priority']
        
        # Mobile annotation (Google 2025 requirement)
        mobile = ET.SubElement(url_elem, f"{{{self.namespaces['mobile']}}}mobile")
        
        # Image information (for blog posts)
        if include_images and page_data.get('images'):
            for img_url in page_data['images']:
                image_elem = ET.SubElement(url_elem, f"{{{self.namespaces['image']}}}image")
                image_loc = ET.SubElement(image_elem, f"{{{self.namespaces['image']}}}loc")
                image_loc.text = img_url
        
        return url_elem

    def validate_sitemap_xml(self, tree):
        """Sitemap XML'ini doƒürula"""
        print("‚úÖ Sitemap XML doƒürulamasƒ±...")
        
        issues = []
        root = tree.getroot()
        
        # URL sayƒ±sƒ± kontrol√º (Google limit: 50,000)
        url_count = len(root.findall('.//url'))
        if url_count > 50000:
            issues.append(f"URL sayƒ±sƒ± limit a≈üƒ±mƒ±: {url_count}/50,000")
        
        # URL uzunluk kontrol√º (max 2048 karakter)
        for url_elem in root.findall('.//url'):
            loc_elem = url_elem.find('loc')
            if loc_elem is not None and len(loc_elem.text) > 2048:
                issues.append(f"URL √ßok uzun: {loc_elem.text[:50]}...")
        
        # Namespace kontrol√º
        required_namespaces = ['http://www.sitemaps.org/schemas/sitemap/0.9']
        for ns in required_namespaces:
            if ns not in tree.getroot().attrib.values():
                issues.append(f"Eksik namespace: {ns}")
        
        if issues:
            print("‚ö†Ô∏è Sitemap doƒürulama sorunlarƒ±:")
            for issue in issues:
                print(f"   - {issue}")
            return False
        else:
            print("‚úÖ Sitemap W3C standartlarƒ±na uygun")
            return True

    def save_sitemap(self, tree):
        """Sitemap'i kaydet"""
        sitemap_path = os.path.join(self.project_root, 'sitemap.xml')
        
        # XML formatƒ±nƒ± g√ºzelle≈ütir
        ET.indent(tree, space="  ", level=0)
        
        # XML declaration ile kaydet
        tree.write(sitemap_path, encoding='utf-8', xml_declaration=True)
        
        return sitemap_path

    def create_github_action_integration(self):
        """GitHub Actions entegrasyonu i√ßin script olu≈ütur"""
        integration_script = '''#!/usr/bin/env python3
"""
GitHub Actions i√ßin Sitemap Otomasyonu
Bu script GitHub Actions workflow'unda √ßalƒ±≈üƒ±r
"""

import sys
import os

# Proje k√∂k dizinini bul
project_root = os.getcwd()
sys.path.append(os.path.join(project_root, 'seo/scripts'))

try:
    from automated_sitemap_integration import AutomatedSitemapManager
    
    def main():
        manager = AutomatedSitemapManager(project_root)
        
        # Blog durumunu kontrol et
        published_blogs = manager.detect_published_blogs()
        print(f"üìä {len(published_blogs)} yayƒ±nlanmƒ±≈ü blog tespit edildi")
        
        # Site sayfalarƒ±nƒ± tespit et
        site_pages = manager.detect_all_site_pages()
        print(f"üìä {len(site_pages)} site sayfasƒ± tespit edildi")
        
        # Sitemap olu≈ütur
        sitemap_tree = manager.create_comprehensive_sitemap(site_pages, published_blogs)
        
        # Doƒürula
        is_valid = manager.validate_sitemap_xml(sitemap_tree)
        if not is_valid:
            print("‚ùå Sitemap doƒürulama ba≈üarƒ±sƒ±z")
            return False
        
        # Kaydet
        sitemap_path = manager.save_sitemap(sitemap_tree)
        print(f"‚úÖ Sitemap g√ºncellendi: {sitemap_path}")
        
        return True
    
    if __name__ == "__main__":
        success = main()
        exit(0 if success else 1)

except ImportError as e:
    print(f"‚ùå Import hatasƒ±: {e}")
    exit(1)
'''
        
        script_path = os.path.join(self.project_root, '.github/scripts/update_sitemap.py')
        os.makedirs(os.path.dirname(script_path), exist_ok=True)
        
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(integration_script)
        
        return script_path

    def update_github_workflow(self):
        """GitHub Actions workflow'unu sitemap otomasyonu ile g√ºncelle"""
        workflow_path = os.path.join(self.project_root, '.github/workflows/blog-automation.yml')
        
        try:
            with open(workflow_path, 'r', encoding='utf-8') as f:
                workflow_content = f.read()
            
            # Sitemap update step'ini ekle
            sitemap_step = '''
    - name: Update Sitemap (Enhanced)
      if: github.event.inputs.blog_action == 'update_sitemap' || github.event.inputs.blog_action == 'full_pipeline' || steps.detect_changes.outputs.has_blog_changes == 'true'
      run: |
        echo "üó∫Ô∏è Dinamik sitemap g√ºncellemesi ba≈ülƒ±yor..."
        python3 .github/scripts/update_sitemap.py
        
        # Sitemap doƒürulama
        if [ -f "sitemap.xml" ]; then
          echo "‚úÖ Sitemap ba≈üarƒ±yla g√ºncellendi"
          
          # Sitemap istatistikleri
          URL_COUNT=$(grep -c "<url>" sitemap.xml)
          echo "üìä Toplam URL: $URL_COUNT"
          
          # Google Search Console bildirimi i√ßin hazƒ±rlƒ±k
          echo "sitemap_url=https://dryallekurutemizleme.com/sitemap.xml" >> $GITHUB_OUTPUT
        else
          echo "‚ùå Sitemap olu≈üturulamadƒ±"
          exit 1
        fi
'''
            
            # Workflow'u g√ºncelle (basit replacement)
            if 'Update Sitemap (Enhanced)' not in workflow_content:
                # Sitemap update b√∂l√ºm√ºn√º bul ve deƒüi≈ütir
                updated_content = workflow_content.replace(
                    '- name: Update Sitemap',
                    '- name: Update Sitemap (Enhanced)'
                )
                
                with open(workflow_path, 'w', encoding='utf-8') as f:
                    f.write(updated_content)
                
                print(f"‚úÖ GitHub workflow g√ºncellendi: {workflow_path}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è GitHub workflow g√ºncelleme hatasƒ±: {str(e)}")

    def generate_sitemap_report(self, site_pages, blog_pages, sitemap_path):
        """Sitemap raporu olu≈ütur"""
        report = {
            "generation_date": datetime.now().isoformat(),
            "sitemap_path": sitemap_path,
            "total_urls": len(site_pages) + len(blog_pages),
            "site_pages": len(site_pages),
            "blog_pages": len(blog_pages),
            "page_breakdown": {},
            "priority_distribution": {},
            "last_updated_pages": []
        }
        
        # Sayfa t√ºr√º daƒüƒ±lƒ±mƒ±
        page_types = {}
        priority_dist = {}
        
        all_pages = site_pages + blog_pages
        for page in all_pages:
            page_type = page.get('type', 'unknown')
            page_types[page_type] = page_types.get(page_type, 0) + 1
            
            if 'priority' in page:
                priority = page['priority']
                priority_dist[priority] = priority_dist.get(priority, 0) + 1
        
        report["page_breakdown"] = page_types
        report["priority_distribution"] = priority_dist
        
        # Son g√ºncellenen sayfalar (5 adet)
        sorted_pages = sorted(all_pages, key=lambda x: x.get('lastmod', ''), reverse=True)
        report["last_updated_pages"] = [
            {'url': p['url'], 'lastmod': p.get('lastmod', '')} 
            for p in sorted_pages[:5]
        ]
        
        # Raporu kaydet
        report_path = os.path.join(self.project_root, 'seo/reports/sitemap_generation_report.json')
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report_path

def main():
    """Otomatik Sitemap Y√∂netimi"""
    print("üó∫Ô∏è OTOMATƒ∞K Sƒ∞TEMAP Y√ñNETƒ∞Mƒ∞")
    print("=" * 50)
    print("üéØ D2: GitHub Actions + Dinamik Sitemap")
    print("=" * 50)
    
    manager = AutomatedSitemapManager()
    
    try:
        # 1. Yayƒ±nlanmƒ±≈ü bloglarƒ± tespit et
        print("üìä Yayƒ±nlanmƒ±≈ü blog analizi...")
        published_blogs = manager.detect_published_blogs()
        print(f"‚úÖ {len(published_blogs)} yayƒ±nlanmƒ±≈ü blog tespit edildi")
        
        # 2. Site sayfalarƒ±nƒ± tespit et
        print("üìä Site sayfalarƒ± analizi...")
        site_pages = manager.detect_all_site_pages()
        print(f"‚úÖ {len(site_pages)} site sayfasƒ± tespit edildi")
        
        # 3. Kapsamlƒ± sitemap olu≈ütur
        sitemap_tree = manager.create_comprehensive_sitemap(site_pages, published_blogs)
        
        # 4. XML doƒürulama
        is_valid = manager.validate_sitemap_xml(sitemap_tree)
        if not is_valid:
            print("‚ùå Sitemap doƒürulama ba≈üarƒ±sƒ±z")
            return False
        
        # 5. Sitemap'i kaydet
        sitemap_path = manager.save_sitemap(sitemap_tree)
        
        # 6. GitHub Actions entegrasyonu
        print("üîß GitHub Actions entegrasyonu...")
        script_path = manager.create_github_action_integration()
        manager.update_github_workflow()
        
        # 7. Rapor olu≈ütur
        report_path = manager.generate_sitemap_report(site_pages, published_blogs, sitemap_path)
        
        # √ñzet
        print("\n" + "=" * 50)
        print("üìä Sƒ∞TEMAP ENTEGRASYONƒ∞ TAMAMLANDI")
        print("=" * 50)
        print(f"‚úÖ Toplam URL: {len(site_pages) + len(published_blogs)}")
        print(f"‚úÖ Site sayfalarƒ±: {len(site_pages)}")
        print(f"‚úÖ Blog sayfalarƒ±: {len(published_blogs)}")
        print(f"‚úÖ Sitemap: {sitemap_path}")
        print(f"‚úÖ GitHub script: {script_path}")
        print(f"‚úÖ Rapor: {report_path}")
        
        print("\nüöÄ √ñZELLƒ∞KLER:")
        print("‚úÖ Otomatik yayƒ±n tespiti (published: true)")
        print("‚úÖ Dinamik √∂ncelik belirleme")
        print("‚úÖ W3C standart uyumluluƒüu")
        print("‚úÖ Google 2025 mobil optimizasyonu")
        print("‚úÖ GitHub Actions entegrasyonu")
        print("‚úÖ G√∂rsel URL'leri dahil")
        
        print("\nüöÄ SONRAKI ADIMLAR:")
        print("1. Google Search Console'a sitemap g√∂nder")
        print("2. GitHub Actions workflow'unu test et")
        print("3. Sitemap performansƒ±nƒ± izle")
        print("4. Otomatik bildirim sistemini kur")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Kritik hata: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)